"""
Local GPU model service using Hugging Face Transformers with comprehensive observability.

This module provides GPU-accelerated text generation capabilities with OpenTelemetry
instrumentation for hardware monitoring, model performance tracking, and inference metrics.
"""

import time
import os
from typing import Dict, List, Optional, Any
import structlog
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from opentelemetry import trace, metrics
from opentelemetry.trace import Status, StatusCode
# Using custom AI span attributes since AI semantic conventions may not be available
class AISpanAttributes:
    LLM_REQUEST_MODEL = "llm.request.model"
    LLM_REQUEST_TEMPERATURE = "llm.request.temperature"
    LLM_REQUEST_MAX_TOKENS = "llm.request.max_tokens"
    LLM_USAGE_PROMPT_TOKENS = "llm.usage.prompt_tokens"
    LLM_USAGE_COMPLETION_TOKENS = "llm.usage.completion_tokens"
    LLM_USAGE_TOTAL_TOKENS = "llm.usage.total_tokens"
import psutil
import GPUtil
from dotenv import load_dotenv

# Initialize observability FIRST
from .observability import run_eval

load_dotenv()

# OpenLIT evaluation imports - create instance later after env is loaded
try:
    from openlit.evals import All
    OPENLIT_AVAILABLE = True
except ImportError:
    All = None
    OPENLIT_AVAILABLE = False

logger = structlog.get_logger(__name__)

# Create OpenLIT evals instance after environment is loaded
openlit_evals = None
if OPENLIT_AVAILABLE and All:
    try:
        openlit_evals = All(collect_metrics=True)
    except Exception as e:
        logger.warning(f"Failed to create OpenLIT evals: {e}")
        OPENLIT_AVAILABLE = False

logger = structlog.get_logger(__name__)
tracer = trace.get_tracer(__name__)
meter = metrics.get_meter(__name__)

# Metrics for model inference
model_inference_duration = meter.create_histogram(
    name="model_inference_duration_seconds",
    description="Duration of local model inference",
    unit="s"
)

model_token_generation = meter.create_counter(
    name="model_tokens_generated_total",
    description="Total tokens generated by local model"
)

model_inference_count = meter.create_counter(
    name="model_inference_total",
    description="Total number of model inferences"
)

model_errors = meter.create_counter(
    name="model_errors_total",
    description="Total number of model inference errors"
)

# Hardware metrics
gpu_utilization = meter.create_gauge(
    name="gpu_utilization_percent",
    description="GPU utilization percentage"
)

gpu_memory_used = meter.create_gauge(
    name="gpu_memory_used_bytes",
    description="GPU memory used in bytes"
)

gpu_temperature = meter.create_gauge(
    name="gpu_temperature_celsius",
    description="GPU temperature in Celsius"
)

cpu_utilization = meter.create_gauge(
    name="cpu_utilization_percent",
    description="CPU utilization percentage"
)

memory_utilization = meter.create_gauge(
    name="memory_utilization_percent",
    description="System memory utilization percentage"
)

class LocalModelService:
    """Local GPU model service with comprehensive observability."""
    
    def __init__(self):
        self.model_name = os.getenv("LOCAL_MODEL_NAME", "microsoft/DialoGPT-medium")
        self.use_gpu = os.getenv("USE_GPU", "true").lower() == "true"
        self.gpu_device_id = int(os.getenv("GPU_DEVICE_ID", "0"))
        
        # Check GPU availability
        self.device = self._setup_device()
        
        # Initialize model and tokenizer (will be set in _load_model)
        self.tokenizer: Any = None  # PreTrainedTokenizer
        self.model: Any = None  # PreTrainedModel
        self.generator: Any = None  # Pipeline
        
        self._load_model()
        
        logger.info(
            "Local model service initialized",
            model_name=self.model_name,
            device=str(self.device),
            gpu_available=torch.cuda.is_available(),
            gpu_count=torch.cuda.device_count() if torch.cuda.is_available() else 0
        )
    
    def _setup_device(self) -> torch.device:
        """Setup the compute device (GPU or CPU)."""
        if self.use_gpu:
            # Prefer NVIDIA CUDA if available
            if torch.cuda.is_available():
                device = torch.device(f"cuda:{self.gpu_device_id}")
                logger.info(f"Using GPU device: {device}")
            # Fall back to Apple Silicon MPS if available
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                device = torch.device("mps")
                logger.info("Using Apple Silicon MPS device")
            else:
                device = torch.device("cpu")
                logger.info("GPU not available; using CPU device")
        else:
            device = torch.device("cpu")
            logger.info("Using CPU device (GPU disabled by config)")
            
        return device
    
    def _load_model(self):
        """Load the model and tokenizer."""
        with tracer.start_as_current_span(
            "model.load",
            attributes={
                "model.name": self.model_name,
                "model.device": str(self.device),
                AISpanAttributes.LLM_REQUEST_MODEL: self.model_name
            }
        ) as span:
            
            try:
                start_time = time.time()
                
                logger.info(f"Loading model: {self.model_name}")
                
                # Load tokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                
                # Add padding token if it doesn't exist
                if self.tokenizer.pad_token is None:  # type: ignore[union-attr]
                    self.tokenizer.pad_token = self.tokenizer.eos_token  # type: ignore[union-attr]
                
                # Load model
                dtype = (
                    torch.float16 if self.device.type in ("cuda", "mps") else torch.float32
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    dtype=dtype,
                    device_map="auto" if self.device.type == "cuda" else None,
                    low_cpu_mem_usage=True
                )
                
                # Move to device for non-CUDA paths (CUDA handled via device_map)
                if self.device.type != "cuda":
                    self.model = self.model.to(self.device)  # type: ignore[union-attr]
                
                # Create pipeline for easier inference
                gen_device = (
                    self.gpu_device_id if self.device.type == "cuda"
                    else (self.device if self.device.type == "mps" else -1)
                )
                gen_dtype = (
                    torch.float16 if self.device.type in ("cuda", "mps") else torch.float32
                )
                self.generator = pipeline(  # type: ignore[call-overload]
                    "text-generation",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    device=gen_device,
                    dtype=gen_dtype
                )
                
                load_duration = time.time() - start_time
                
                # Get model size information
                model_size = sum(p.numel() for p in self.model.parameters())  # type: ignore[union-attr]
                
                span.set_attributes({
                    "model.load_duration_seconds": load_duration,
                    "model.parameter_count": model_size,
                    "model.device_final": str(self.device)
                })
                
                span.set_status(Status(StatusCode.OK))
                
                logger.info(
                    "Model loaded successfully",
                    model_name=self.model_name,
                    device=str(self.device),
                    parameter_count=model_size,
                    load_duration=load_duration
                )
                
            except Exception as e:
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                logger.error(f"Failed to load model: {e}")
                raise
    
    def _collect_hardware_metrics(self):
        """Collect and record hardware metrics."""
        try:
            # CPU metrics
            cpu_percent = psutil.cpu_percent(interval=0.1)
            cpu_utilization.set(
                cpu_percent, 
                attributes={
                    "device": "cpu",
                    "telemetry_sdk_name": "opentelemetry"
                }
            )
            
            # Memory metrics
            memory = psutil.virtual_memory()
            memory_utilization.set(
                memory.percent, 
                attributes={
                    "device": "system_memory",
                    "telemetry_sdk_name": "opentelemetry"
                }
            )
            
            # GPU metrics (if available)
            if torch.cuda.is_available():
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus and len(gpus) > self.gpu_device_id:
                        gpu = gpus[self.gpu_device_id]
                        
                        gpu_utilization.set(
                            gpu.load * 100,
                            attributes={
                                "device": f"gpu_{self.gpu_device_id}",
                                "gpu_name": gpu.name,
                                "telemetry_sdk_name": "opentelemetry"
                            }
                        )
                        
                        gpu_memory_used.set(
                            gpu.memoryUsed * 1024 * 1024,  # Convert MB to bytes
                            attributes={
                                "device": f"gpu_{self.gpu_device_id}",
                                "gpu_name": gpu.name,
                                "telemetry_sdk_name": "opentelemetry"
                            }
                        )
                        
                        gpu_temperature.set(
                            gpu.temperature,
                            attributes={
                                "device": f"gpu_{self.gpu_device_id}",
                                "gpu_name": gpu.name,
                                "telemetry_sdk_name": "opentelemetry"
                            }
                        )
                        
                except Exception as e:
                    logger.warning(f"Failed to collect GPU metrics: {e}")
            # Apple Silicon MPS metrics (limited availability)
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                try:
                    # Utilization is not exposed via PyTorch for MPS; report 0 as placeholder
                    gpu_utilization.set(
                        0.0,
                        attributes={
                            "device": "mps",
                            "gpu_name": "Apple MPS",
                            "telemetry_sdk_name": "opentelemetry"
                        }
                    )

                    # Memory usage available via torch.mps APIs on recent PyTorch versions
                    mps_current = None
                    mps_driver = None
                    try:
                        # Current process allocated memory
                        mps_current = getattr(torch.mps, "current_allocated_memory", None)
                        if callable(mps_current):
                            mps_current = mps_current()
                        else:
                            mps_current = None
                    except Exception:
                        mps_current = None

                    try:
                        mps_driver = getattr(torch.mps, "driver_allocated_memory", None)
                        if callable(mps_driver):
                            mps_driver = mps_driver()
                        else:
                            mps_driver = None
                    except Exception:
                        mps_driver = None

                    if isinstance(mps_current, (int, float)) and mps_current >= 0:
                        gpu_memory_used.set(
                            float(mps_current),
                            attributes={
                                "device": "mps",
                                "gpu_name": "Apple MPS",
                                "memory_source": "current_allocated",
                                "telemetry_sdk_name": "opentelemetry"
                            }
                        )

                    if isinstance(mps_driver, (int, float)) and mps_driver >= 0:
                        gpu_memory_used.set(
                            float(mps_driver),
                            attributes={
                                "device": "mps",
                                "gpu_name": "Apple MPS",
                                "memory_source": "driver_allocated",
                                "telemetry_sdk_name": "opentelemetry"
                            }
                        )
                except Exception as e:
                    logger.warning(f"Failed to collect MPS metrics: {e}")
                    
        except Exception as e:
            logger.warning(f"Failed to collect hardware metrics: {e}")
    
    async def generate_text(
        self,
        prompt: str,
        max_length: int = 100,
        temperature: float = 0.7,
        num_return_sequences: int = 1,
        user_id: Optional[str] = None,
        reference_output: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate text using the local model.
        
        Args:
            prompt: Input prompt for text generation
            max_length: Maximum length of generated text
            temperature: Sampling temperature
            num_return_sequences: Number of sequences to generate
            user_id: User identifier for tracking
            reference_output: Reference output for evaluation
            
        Returns:
            Dict containing generated text and metadata
        """
        start_time = time.time()
        
        with tracer.start_as_current_span(
            "model.generate_text",
            attributes={
                AISpanAttributes.LLM_REQUEST_MODEL: self.model_name,
                AISpanAttributes.LLM_REQUEST_TEMPERATURE: temperature,
                AISpanAttributes.LLM_REQUEST_MAX_TOKENS: max_length,
                "model.prompt_length": len(prompt),
                "model.num_return_sequences": num_return_sequences,
                "model.user_id": user_id or "unknown",
                "model.device": str(self.device)
            }
        ) as span:
            
            try:
                # Collect hardware metrics before inference
                self._collect_hardware_metrics()
                
                logger.info(
                    "Starting text generation",
                    model=self.model_name,
                    prompt_length=len(prompt),
                    max_length=max_length,
                    temperature=temperature,
                    user_id=user_id
                )
                
                # Tokenize input
                input_tokens = self.tokenizer.encode(prompt, return_tensors="pt")  # type: ignore[union-attr]
                input_token_count = input_tokens.shape[1]  # type: ignore[union-attr]
                
                # Generate text
                with torch.no_grad():
                    outputs = self.generator(  # type: ignore[misc]
                        prompt,
                        max_length=max_length,
                        temperature=temperature,
                        num_return_sequences=num_return_sequences,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id,  # type: ignore[union-attr]
                        truncation=True
                    )
                
                # Process outputs
                generated_texts = []
                total_output_tokens = 0
                
                for output in outputs:  # type: ignore[union-attr]
                    generated_text = output["generated_text"]  # type: ignore[index, call-overload]
                    # Remove the prompt from the generated text
                    if generated_text.startswith(prompt):
                        generated_text = generated_text[len(prompt):].strip()
                    
                    generated_texts.append(generated_text)
                    
                    # Count tokens in generated text
                    output_tokens = self.tokenizer.encode(generated_text, return_tensors="pt")  # type: ignore[union-attr]
                    total_output_tokens += output_tokens.shape[1]  # type: ignore[union-attr]
                
                duration = time.time() - start_time
                
                # Collect hardware metrics after inference
                self._collect_hardware_metrics()
                
                # Update span attributes
                span.set_attributes({
                    AISpanAttributes.LLM_USAGE_PROMPT_TOKENS: input_token_count,
                    AISpanAttributes.LLM_USAGE_COMPLETION_TOKENS: total_output_tokens,
                    AISpanAttributes.LLM_USAGE_TOTAL_TOKENS: input_token_count + total_output_tokens,
                    "model.generation_duration_seconds": duration,
                    "model.tokens_per_second": total_output_tokens / duration if duration > 0 else 0
                })
                
                # Record metrics
                model_inference_duration.record(
                    duration,
                    attributes={
                        "model": self.model_name,
                        "device": str(self.device),
                        "user_id": user_id or "unknown",
                        "telemetry_sdk_name": "opentelemetry"
                    }
                )
                
                model_token_generation.add(
                    total_output_tokens,
                    attributes={
                        "model": self.model_name,
                        "device": str(self.device),
                        "user_id": user_id or "unknown",
                        "telemetry_sdk_name": "opentelemetry"
                    }
                )
                
                model_inference_count.add(
                    1,
                    attributes={
                        "model": self.model_name,
                        "device": str(self.device),
                        "status": "success",
                        "user_id": user_id or "unknown",
                        "telemetry_sdk_name": "opentelemetry"
                    }
                )
                
                span.set_status(Status(StatusCode.OK))
                
                logger.info(
                    "Text generation completed",
                    model=self.model_name,
                    input_tokens=input_token_count,
                    output_tokens=total_output_tokens,
                    duration=duration,
                    tokens_per_second=total_output_tokens / duration if duration > 0 else 0,
                    sequences_generated=len(generated_texts)
                )
                
                # --- Eval Instrumentation ---
                from .observability import run_eval
                # For demo: use prompt as reference if reference_output not provided
                eval_result = run_eval(generated_texts[0], reference_output or prompt, user_id=user_id,
                                     system="transformers", operation="text_generation", model=self.model_name)
                
                # OpenLIT evaluation (LLM-as-a-Judge)
                openlit_eval_result = None
                if OPENLIT_AVAILABLE and openlit_evals and generated_texts[0]:
                    try:
                        # Provide the prompt as context for evaluation
                        contexts = [prompt] if prompt else None
                        openlit_eval_result = openlit_evals.measure(
                            prompt=prompt,
                            contexts=contexts,
                            text=generated_texts[0]
                        )
                        logger.info("OpenLIT evaluation completed", result=openlit_eval_result)
                    except Exception as e:
                        logger.warning("OpenLIT evaluation failed", error=str(e))

                return {
                    "generated_texts": generated_texts,
                    "model": self.model_name,
                    "usage": {
                        "prompt_tokens": input_token_count,
                        "completion_tokens": total_output_tokens,
                        "total_tokens": input_token_count + total_output_tokens
                    },
                    "performance": {
                        "duration_seconds": duration,
                        "tokens_per_second": total_output_tokens / duration if duration > 0 else 0
                    },
                    "device": str(self.device),
                    "eval": eval_result,
                    "openlit_eval": openlit_eval_result
                }
                
            except Exception as e:
                duration = time.time() - start_time
                
                # Record error metrics
                model_errors.add(
                    1,
                    attributes={
                        "model": self.model_name,
                        "device": str(self.device),
                        "error_type": type(e).__name__,
                        "user_id": user_id or "unknown",
                        "telemetry_sdk_name": "opentelemetry"
                    }
                )
                
                model_inference_count.add(
                    1,
                    attributes={
                        "model": self.model_name,
                        "device": str(self.device),
                        "status": "error",
                        "user_id": user_id or "unknown",
                        "telemetry_sdk_name": "opentelemetry"
                    }
                )
                
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                
                logger.error(
                    "Text generation failed",
                    model=self.model_name,
                    error=str(e),
                    error_type=type(e).__name__,
                    duration=duration
                )
                
                raise
    
    async def get_model_info(self) -> Dict[str, Any]:
        """Get information about the loaded model."""
        with tracer.start_as_current_span("model.get_info") as span:
            
            try:
                model_size = sum(p.numel() for p in self.model.parameters()) if self.model else 0  # type: ignore[union-attr]
                
                # Get GPU info if available
                gpu_info = {}
                if torch.cuda.is_available():
                    try:
                        gpus = GPUtil.getGPUs()
                        if gpus and len(gpus) > self.gpu_device_id:
                            gpu = gpus[self.gpu_device_id]
                            gpu_info = {
                                "name": gpu.name,
                                "memory_total": gpu.memoryTotal,
                                "memory_used": gpu.memoryUsed,
                                "memory_free": gpu.memoryFree,
                                "temperature": gpu.temperature,
                                "utilization": gpu.load * 100
                            }
                    except Exception as e:
                        logger.warning(f"Failed to get GPU info: {e}")
                
                info = {
                    "model_name": self.model_name,
                    "device": str(self.device),
                    "parameter_count": model_size,
                    "gpu_available": torch.cuda.is_available(),
                    "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
                    "gpu_info": gpu_info,
                    "cpu_count": psutil.cpu_count(),
                    "memory_total": psutil.virtual_memory().total
                }
                
                span.set_attributes({
                    "model.parameter_count": model_size,
                    "model.device": str(self.device)
                })
                
                return info
                
            except Exception as e:
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                logger.error(f"Failed to get model info: {e}")
                raise

# Global service instance
local_model_service = LocalModelService()